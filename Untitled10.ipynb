{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fde6ab7c-d8d9-4c09-91b4-3f20a2946daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import math\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52e29bf9-dcc6-411f-9a3e-8d7cb7dc8f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1' \n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dc73d20-9708-4e75-90ef-8f5b11be3c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(prompt_file, story_file, max_length=512, dataset_size=None, chunk_size=1024):\n",
    "    def clean_punctuation(text):\n",
    "        for p in '!,.:;?':\n",
    "            text = text.replace(' ' + p, p)\n",
    "        text = text.replace(' ' + 'n\\'t', 'n\\'t')\n",
    "        text = text.replace(' ' + '\\'s', '\\'s')\n",
    "        return text\n",
    "\n",
    "    def read_file_chunks(file_path, chunk_size):\n",
    "        with open(file_path, encoding='utf-8-sig') as file:  # Change encoding to 'utf-8-sig'\n",
    "            while True:\n",
    "                chunk = file.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                yield chunk\n",
    "\n",
    "    prompts = open(prompt_file, encoding='utf-8-sig').readlines()  # Change encoding to 'utf-8-sig'\n",
    "    stories = []\n",
    "\n",
    "    for chunk in read_file_chunks(story_file, chunk_size):\n",
    "        stories.extend(chunk.splitlines())\n",
    "\n",
    "    if dataset_size:\n",
    "        prompts = prompts[:dataset_size]\n",
    "        stories = stories[:dataset_size]\n",
    "\n",
    "    data = []\n",
    "    for prompt, story in zip(prompts, stories):\n",
    "        combined_text = prompt.strip() + ' <sep> ' + \" \".join(story.split()[:300])\n",
    "        cleaned_text = clean_punctuation(combined_text)\n",
    "        data.append(cleaned_text[:max_length])\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9e6140c-9899-44b5-bfc3-cd6e354d6e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_file_train = 'train.wp_target'\n",
    "story_file_train = 'train.wp_source'\n",
    "prompt_file_valid = 'valid.wp_target'\n",
    "story_file_valid = 'valid.wp_target'\n",
    "prompt_file_test = 'test.wp_target'\n",
    "story_file_test = 'test.wp_source'\n",
    "\n",
    "# Load a reduced dataset size (e.g., 100 for training, 20 for validation)\n",
    "train_text = load_data(prompt_file_train, story_file_train, dataset_size=100)\n",
    "valid_text = load_data(prompt_file_valid, story_file_valid, dataset_size=20)\n",
    "test_text = load_data(prompt_file_test, story_file_test, dataset_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53bdd8ef-2732-4fa9-ac28-73ef2b509c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "inputs_train = tokenizer(train_text, padding=True, truncation=True, max_length=512)\n",
    "inputs_valid = tokenizer(valid_text, padding=True, truncation=True, max_length=512)\n",
    "inputs_test = tokenizer(test_text, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "labels_train = [ids[:sum(mask)] + [-100] * (len(mask) - sum(mask)) for ids, mask in zip(inputs_train['input_ids'], inputs_train['attention_mask'])]\n",
    "labels_valid = [ids[:sum(mask)] + [-100] * (len(mask) - sum(mask)) for ids, mask in zip(inputs_valid['input_ids'], inputs_valid['attention_mask'])]\n",
    "labels_test = [ids[:sum(mask)] + [-100] * (len(mask) - sum(mask)) for ids, mask in zip(inputs_test['input_ids'], inputs_test['attention_mask'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5800e27c-942e-456d-bb8c-d198a9e7aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.ids = inputs['input_ids']\n",
    "        self.attention_mask = inputs['attention_mask']\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.ids[item], dtype=torch.long), torch.tensor(self.attention_mask[item], dtype=torch.long), torch.tensor(self.labels[item], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2ba2d7b-a3f9-47db-a7cd-402e6e385323",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = StoryDataset(inputs_train, labels_train)\n",
    "valid_dataset = StoryDataset(inputs_valid, labels_valid)\n",
    "test_dataset = StoryDataset(inputs_test, labels_test)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=2)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, shuffle=False, batch_size=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab45431a-bc18-4b7b-b368-d50af163f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "total_num_training_steps = len(train_dataloader) * 1  # Assuming 1 epoch\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96e72368-05df-492a-a339-72bdcb0e3b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss for Epoch 1: 3.622979373931885\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for inputs in train_dataloader:\n",
    "        input_ids, attention_mask, labels = [x for x in inputs]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    average_train_loss = train_loss / len(train_dataloader)\n",
    "    print(f'Average training loss for Epoch {epoch + 1}: {average_train_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "002205fc-9582-4d80-b685-2e13e021a50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation loss: 3.5467012882232667\n",
      "Perplexity for the validation dataset: 34.69866758966632\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_loss = []\n",
    "\n",
    "for inputs in valid_dataloader:\n",
    "    input_ids, attention_mask, labels = [x for x in inputs]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        batch_loss = outputs.loss\n",
    "\n",
    "    eval_loss.append(batch_loss.cpu().item())\n",
    "\n",
    "average_eval_loss = sum(eval_loss) / len(valid_dataloader)\n",
    "perplexity = math.exp(average_eval_loss)\n",
    "print(f'Average validation loss: {average_eval_loss}')\n",
    "print(f'Perplexity for the validation dataset: {perplexity}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "236b9df2-fd83-4307-acfd-242888f1de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_widget = widgets.Text(\n",
    "    placeholder='Enter your prompt',\n",
    "    description='Prompt:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(description='Generate Stories')\n",
    "output_area = widgets.Output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9ca6fbe-8b06-4d5d-9458-d36b8e3e5723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_button_click(b):\n",
    "    user_prompt = user_prompt_widget.value.strip()\n",
    "\n",
    "    if not user_prompt:\n",
    "        with output_area:\n",
    "            print(\"Prompt cannot be empty. Please enter a prompt.\")\n",
    "        return\n",
    "\n",
    "    encoded_user_prompt = tokenizer.encode(user_prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=encoded_user_prompt,\n",
    "        max_length=300,\n",
    "        temperature=0.8,\n",
    "        top_k=30,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.0,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "    if len(output_sequences.shape) > 2:\n",
    "        output_sequences.squeeze_()\n",
    "\n",
    "    generated_story = \"\"\n",
    "    for generated_sequence in output_sequences:\n",
    "        generated_sequence = generated_sequence.tolist()\n",
    "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "        text = text[: text.find(tokenizer.eos_token)]\n",
    "        generated_story += text\n",
    "\n",
    "    output_area.clear_output(wait=True)\n",
    "    with output_area:\n",
    "        print(\"\\nGenerated Story:\")\n",
    "        print(generated_story)\n",
    "\n",
    "generate_button.on_click(generate_button_click)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79a9a31a-da85-4a4b-ba7e-9c4b983ed25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5866dd52f72f43eeac19a283eded7842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Prompt:', placeholder='Enter your prompt')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b33cfeb0b446d2915da9085a37b2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Stories', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9773dc4252466fb1d50ff757df57d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(user_prompt_widget)\n",
    "display(generate_button)\n",
    "display(output_area)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
